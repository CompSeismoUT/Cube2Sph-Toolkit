
! elastic solver


subroutine compute_forces_viscoelastic_ADE_GPU_calling()
  use specfem_par
  use specfem_par_acoustic
  use specfem_par_elastic
  use specfem_par_poroelastic
  use pml_par

  !nqdu added
  use wavefield_discontinuity_par,only: IS_WAVEFIELD_DISCONTINUITY

  implicit none

  !local 
  integer:: iphase
  logical :: ADE_CONTRIB
  
  ADE_CONTRIB = PML_CONDITIONS .and. USE_ADE_PML.and.(nglob_CPML>0)

  !! Tianshi Liu: for solving wavefield discontinuity problem with
  !! non-split-node scheme
  !nqdu if (IS_WAVEFIELD_DISCONTINUITY) then
  if (IS_WAVEFIELD_DISCONTINUITY .and. COUPLE_WITH_INJECTION_TECHNIQUE) then
    call read_wavefield_discontinuity_file()
    call transfer_wavefield_discontinuity_to_GPU()
  endif

  do iphase = 1,2 
    ! contains both forward SIM_TYPE==1 and backward SIM_TYPE==3 simulations
    call compute_forces_viscoelastic_cuda_ade(Mesh_pointer, iphase, deltat, &
                                          nspec_outer_elastic, &
                                          nspec_inner_elastic, &
                                          COMPUTE_AND_STORE_STRAIN,ATTENUATION,ANISOTROPY)

    ! computes additional contributions
    if(iphase == 1)then

      if (IS_WAVEFIELD_DISCONTINUITY .and. COUPLE_WITH_INJECTION_TECHNIQUE) then
        call add_traction_discontinuity_GPU()
      endif
    
      ! adds elastic absorbing boundary term to acceleration (Stacey conditions)
      if (STACEY_ABSORBING_CONDITIONS) then
        call compute_stacey_viscoelastic_GPU(iphase,num_abs_boundary_faces, &
                                            SIMULATION_TYPE,SAVE_FORWARD,NSTEP,it, &
                                            b_num_abs_boundary_faces,b_reclen_field,b_absorb_field, &
                                            Mesh_pointer)
      endif

      ! acoustic coupling
      if (ACOUSTIC_SIMULATION) then
        if (num_coupling_ac_el_faces > 0) then
          call compute_coupling_el_ac_cuda(Mesh_pointer,iphase, &
                                          num_coupling_ac_el_faces)
        endif
      endif

      ! poroelastic coupling
      if (POROELASTIC_SIMULATION) then
        ! note:
        ! these routines are not implemented as CUDA kernels, we just transfer the fields
        ! from the GPU to the CPU and vice versa

        ! transfers displacement & acceleration to the CPU
        call transfer_displ_from_device(NDIM*NGLOB_AB,displ, Mesh_pointer)
        call transfer_accel_from_device(NDIM*NGLOB_AB,accel, Mesh_pointer)

        call compute_coupling_viscoelastic_po(iphase)

        ! transfers acceleration back to GPU
        call transfer_accel_to_device(NDIM*NGLOB_AB,accel, Mesh_pointer)
      endif

      ! adds source term (single-force/moment-tensor solution)
      ! note: we will add all source contributions in the first pass, when iphase == 1
      !       to avoid calling the same routine twice and to check if the source element is an inner/outer element
      !call transfer_accel_from_device(NGLOB_AB*NDIM,accel,Mesh_pointer)
      call compute_add_sources_viscoelastic_GPU()
    endif 

    ! communication
#ifndef USE_CUDA_AWARE_MPI
    if(iphase == 1) then  
      ! copy accel boundary to accel->d_send_buffer -> (async) h_send_buf
      call sync_accel_bdry_buffers(Mesh_pointer,iphase,buffer_send_vector_ext_mesh)
      call assemble_MPI_vector_send_cuda(NPROC, &
                  buffer_send_vector_ext_mesh,buffer_recv_vector_ext_mesh, &
                  num_interfaces_ext_mesh,max_nibool_interfaces_ext_mesh, &
                  nibool_interfaces_ext_mesh, &
                  my_neighbors_ext_mesh, &
                  request_send_vector_ext_mesh,request_recv_vector_ext_mesh)
        
      if(ADE_CONTRIB) then 
        call sync_ade_bdry_buffers(Mesh_pointer,iphase,buffer_send_matrix_PML)
        call assemble_MPI_matrix_send_cuda(NPROC,buffer_send_matrix_PML,&
                                          buffer_recv_matrix_PML,num_interfaces_PML,&
                                          max_nibool_interfaces_PML,nibool_interfaces_PML,&
                                          my_neighbors_PML,&
                                          request_send_matrix_PML,request_recv_matrix_PML)
      endif
    
    else 
      call assemble_MPI_accel_update(NPROC,Mesh_pointer,num_interfaces_ext_mesh,&
                                     max_nibool_interfaces_ext_mesh,&
                                     buffer_recv_vector_ext_mesh,&
                                     request_send_vector_ext_mesh,&
                                     request_recv_vector_ext_mesh)

      !ADE Qt_t
      if(ADE_CONTRIB)  then
        call assemble_MPI_ADE_update(NPROC,Mesh_pointer,num_interfaces_PML,&
                                  max_nibool_interfaces_PML,&
                                  buffer_recv_matrix_PML,&
                                  request_send_matrix_PML,&
                                  request_recv_matrix_PML)
      endif
    endif
#else  
    ! copy accel boundary to accel->d_send_buffer -> (async) h_send_buf
    call sync_accel_bdry_buffers(Mesh_pointer,iphase,my_neighbors_ext_mesh,&
                                 nibool_interfaces_ext_mesh)
    if(ADE_CONTRIB) then 
      call sync_ade_bdry_buffers(Mesh_pointer,iphase,my_neighbors_PML,&
                                nibool_interfaces_PML)
    endif
#endif
  enddo      

  ! corrector to update PML auxiliary variables
  if(ADE_CONTRIB)  then
    call include_adepml_accel_aux_GPU() 
  endif 

  ! multiplies with inverse of mass matrix (note: rmass has been inverted already)
  ! dirichlet boundary condition has been added to rmass
  call apply_massmat_device(Mesh_pointer)

  ! update velocity
  call update_velocity_device(Mesh_pointer,deltatover2)

  if(ADE_CONTRIB)  then
    call update_Qt_conv_GPU()
    call update_Qu_conv_GPU()
  endif
end subroutine compute_forces_viscoelastic_ADE_GPU_calling